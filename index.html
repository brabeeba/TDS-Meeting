<!DOCTYPE html>
<html lang="en">
	<head>
    	<meta charset="utf-8">
    	<title>TDS Meeting</title>
    	<meta name="viewport" content="width=device-width, initial-scale=1.0">
    	<meta name="description" content="">
    	<meta name="author" content="">

    	<!-- Le styles -->
    	<link href="../personal_site/css/bootstrapFlatly.min.css" rel="stylesheet">
    	<style>
      	body {
        	/*padding-top: 60px;  60px to make the container go all the way to the bottom of the topbar */
      	}
    	</style>
    	<link href="../personal_site/css/bootstrap-responsive.min.css" rel="stylesheet">
    	<link href="../personal_site/css/main.css" rel="stylesheet">
	</head>
	<body>
	<div class="container">
	<div class="row-fluid">
	  <h3>TDS Meeting</h3>
   </div>
	<div class="row-fluid voffset1">
      	<span class="icon-time"></span><strong>Time:</strong> Fridays 12am-1:30pm, Fall Semester 2020 (first Meeting 9/4)
	</div>
	 <div class="row-fluid voffset1">
	  <span class="icon-map-marker"></span><strong>Location:</strong> Over Zoom <br>
	  
	</div>
	 <div class="row-fluid voffset1">
	  <span class="icon-envelope"></span><strong>Organizers:</strong> Nancy Lynch (lynch at csail dot mit dot edu)<br>
	</div>
<div class="row-fluid voffset1">
	<div class="col-md-10 colp-lg-8" >
   		<img src="./triple.png" class="img-rounded img-responsive" alt="my photo">
</div>
  <div class="row voffset1">
    <div class="col-md-12">
      <section id="publications">
      	

        <h3>(Tentative) Schedule</h3>
		
		<p>(9/4) Informal meeting of TDS group members.:  Nancy Lynch</p>
			<ul>
		<li>Every presents what they are working on.</li>
		<li>Explain the format of the TDS meeting</li>
		</ul>
		
		<p>(9/11) Spiking Neural Networks Through the Lens of Streaming Algorithms:  Yael Hitron</p>
			<ul>
			<strong>Abstract:</strong>
			<p>We initiate the study of biological neural networks from the perspective of streaming algorithms.  Like computers, human brains suffer from memory limitations which pose a significant obstacle when processing large scale and dynamically changing data. In computer science, these challenges are captured by the well-known streaming model, which can be traced back to Munro and Paterson ‘78 and has had significant impact in theory and beyond. In the classical streaming setting, one must compute some function f of a stream of updates S={u1,...,um}, given restricted single-pass access to the stream. The primary complexity measure is the space used by the algorithm.</p> 

			<p>In contrast to the large body of work on streaming algorithms, relatively little is known about the computational aspects of data processing in biological neural networks. In this work, we seek to connect these two models, leveraging techniques developed in for streaming algorithms to better understand neural computation. In particular, we consider the spiking neural network model, a distributed model of biological networks in which nodes (neurons) are connected by edges (synapses), and communicate with their neighbors via spiking (i.e., firing). Our primary goal is to design networks for various computational tasks using as few auxiliary (non-input or output) neurons as possible. The number of auxiliary neurons can be thought of as the ‘space’ required by the network.</p>

		<p>Previous algorithmic work in spiking neural networks has many similarities with streaming algorithms. However, the connection between these two space-limited models has not been formally addressed. We take the first steps towards understanding this connection. On the upper bound side, we design neural algorithms based on known streaming algorithms for fundamental tasks, including distinct elements, approximate median, heavy hitters, and more. The number of neurons in our neural solutions almost matches the space bounds of the corresponding streaming algorithms. As a general algorithmic primitive, we show how to implement the important streaming technique of linear sketching efficiently in spiking neural networks. On the lower bound side, we give a generic reduction, showing that any space-efficient spiking neural network can be simulated by a space-efficient streaming algorithm. This reduction lets us translate streaming-space lower bounds into nearly-matching neural-space lower bounds, establishing a close connection between these two models. </p>
		<strong>Slide:</strong>
		<li><a href="./streaming_neural.pdf">Slide</a></li>
		</ul>

		<p>(9/18) Striosomes Selectively Mediate Value-Based Learning Possibly Through Fast Spiking Inhibitory Interneurons: Sabrina Drammis, Jiajia</p>
			<ul>
			<strong>Abstract:</strong>
			<p>Learning valence-based responses to favorable and unfavorable options requires judgments of the relative value of the options and is necessary for species survival. We have found, using engineered mice, that circuit connectivity and function of the striosome compartment of the striatum are critical for this type of learning. Calcium imaging during valence-based learning exhibited a selective correlation between learning and striosomal, but not matrix, signals. This striosomal activity encoded discrimnitation learning and correlated with task engagement. Striosomal function during discrimination learning was distributed in aging, and severely so in a mouse model of Huntington's disease. Anatomical and functional connectivity of parvalbumin-positive, putative fast-spiking interneurons (FSIs) to striatal projection neurons was enhanced in striosomes compared to matrix in mice that learned. Based on our computation model, we suggest that FSIs can modulated striosomal signal-to-noise ratio, crucial for discrimination and learning.</p> 
			<strong>Slide:</strong>
		<li><a href="./striatum.pdf">Slide</a></li>
		</ul>

		<p>(10/2) A Comprehensive and Predictive Agent-based Model for Collective House-Hunting in Ant Colonies: Jiajia</p>
			<ul>
			<strong>Abstract:</strong>
			<p>The decentralized cognition of animal groups is both a challenging biological problem and a potential basis for bio-inspired design. The understanding of these systems and their application can benefit from modeling and analysis of the underlying algorithms. In this study, we define a modeling framework that can be used to formally represent all components of such algorithms. As an example application of the framework, we adapt to it the much-studied house-hunting algorithm used by emigrating colonies of *Temnothorax* ants to reach consensus on a new nest. We provide a Python simulator that encodes accurate individual behavior rules and produces simulated behaviors consistent with empirical observations, on both the individual and group levels. Our model successfully reproduces experimental results showing the high cognitive capacity of colonies, their rational time investment during decision-making, and their ability to avoid and repair splits with the help of social information. We also use the model to make predictions about several unstudied aspects of emigration behavior. The results suggest the value of individual sensitivity to site population for ensuring consensus, and they indicate a more complex relationship between individual behavior and the speed/accuracy trade-off than previously appreciated. The model proved relatively weak at resolving colony divisions among multiple sites, suggesting either limits to the ants' ability to reach consensus, or an aspect of their behavior not captured in our model. It is our hope that these insights and predictions can inspire further research from both the biology and computer science community.</p> 
			<strong>Slide:</strong>
		<li><a href="https://docs.google.com/presentation/d/1GaP9RxY0jdB0vP8hYFIz6k93B3W5bHYulpW71gbUTd8/edit#slide=id.p">Slide</a></li>
		<strong>Paper:</strong>
		<li><a href="https://www.biorxiv.org/content/10.1101/2020.10.07.328047v1">Paper</a></li>
			
		</ul>

		<p>(10/16) How to Color a French Flag: Biologically Inspired Algorithms for Scale-Invariant Patterning: Bertie Ancona</p>
			<ul>
			<strong>Abstract:</strong>
			<p> In the French flag problem, initially uncolored cells on a grid must differentiate to become blue, white or red. The goal is for the cells to color the grid as a French flag, i.e., a three-colored triband, in a distributed manner. To solve a generalized version of the problem in a distributed computational setting, we consider two models: a biologically-inspired version that relies on morphogens (diffusing proteins acting as chemical signals) and a more abstract version based on reliable message passing between cellular agents.</p>
			<p> Much of developmental biology research has focused on concentration-based approaches using morphogens, since morphogen gradients are thought to be an underlying mechanism in tissue patterning. We show that both our model types easily achieve a French ribbon - a French flag in the 1D case. However, extending the ribbon to the 2D flag in the concentration model is somewhat difficult unless each agent has additional positional information. Assuming that cells are identical, it is impossible to achieve a French flag or even a close approximation. In contrast, using a message-based approach in the 2D case only requires assuming that agents can be represented as constant-size state machines.</p> 
			
		</ul>

		<p>(11/13) Network Decomposition and Distributed Derandomization: Mohsen Ghaffari</p>
			<ul>
			<strong>Abstract:</strong>
			<p>  I will provide an overview of a recent line of work [RozhoÅˆ
and Ghaffari at STOC 2020; Ghaffari, Harris, and Kuhn at FOCS 2018;
and Ghaffari, Kuhn, and Maus at STOC 2017], which presented the first
efficient deterministic network decomposition algorithm as well as a
general derandomization result for distributed graph algorithms.
Informally, the derandomization result shows that any
(locally-checkable) graph problem that admits an efficient randomized
distributed algorithm also admits an efficient deterministic
distributed algorithm. These results resolve several central and
decades-old open problems in distributed graph algorithms.</p> 
			<strong>Reading List:</strong>
			<li><a href="https://arxiv.org/abs/1907.10937">Vaclav Rozhon and Mohsen Ghaffari. Polylogarithmic-Time Deterministic Network Decomposition and Distributed Derandomization. ACM Symposium on Theory of Computing (STOC) 2020.</a> </li>
			<li><a href="https://arxiv.org/abs/1711.02194">Mohsen Ghaffari, David Harris, and Fabian Kuhn. On Derandomizing Local Distributed Algorithms. IEEE Symposium on Foundations of Computer Science (FOCS) 2018.</a> </li>
			<li><a href="https://arxiv.org/abs/1611.02663">Mohsen Ghaffari, Fabian Kuhn, and Yannic Maus. On the Complexity of Local Distributed Graph Problems. ACM Symposium on Theory of Computing (STOC) 2017.</a> </li>
			
		</ul>

			<p>(12/4) Locally Solvable Tasks and the Limitations of Valency Arguments: Hagit Attiya, Armando Castañeda, Sergio Rajsbaum</p>
			<ul>
			<strong>Abstract:</strong>
			<p>  An elegant strategy for proving impossibility results in distributed 
computing was introduced in the celebrated FLP consensus impossibility 
proof. This strategy is local in nature as at each stage, one 
configuration of a hypothetical protocol for consensus is considered, 
together with future valencies of possible extensions. This local 
nature makes the strategy very flexible, and has been used in numerous 
situations related to consensus. Hence, one would like to use it for 
impossibility results of two other well-known tasks: set agreement and 
renaming. This paper provides an  explanation of why impossibility 
proofs of these tasks have been of  a global nature. It shows that a 
protocol can always solve such tasks locally, in the following sense. 
Given a configuration and all its future valencies, if a single 
successor configuration is selected, then the protocol can reveal all 
decisions in this branch of executions, satisfying the task 
specification. This result is shown for both set agreement and 
renaming, implying that there are no local impossibility proofs for 
these tasks.</p> 
			<li><a href="./local-solvability-long.pdf">Slide</a> </li>
			<li><a href="https://arxiv.org/abs/2011.10436">Paper</a> </li>
		</ul>

		<p>(2/19) SNOW Revisited: Understanding When Ideal READ Transactions Are Possible: Kishori Konwar</p>
			<ul>
			<strong>Abstract:</strong>
			<p> READ transactions that read data distributed across servers 
dominate the workloads of real-world distributed storage systems. The 
SNOW Theorem stated that ideal READ transactions that have optimal 
latency and the strongest guarantees— i.e., “SNOW” READ 
transactions—are impossible in one specific setting that requires 
three or more clients: at least two readers and one writer. However, 
it left many open questions.  We close all of these open questions 
with new impossibility results and new algorithms.</p>
<p>First, we prove rigorously the original result saying that it is 
impossible to have a READ transactions system that satisfies SNOW 
properties with three or more clients. The insight we gained from this 
proof led to teasing out the implicit assumptions that are required to 
state the results and also, resolving the open question regarding the 
possibility of SNOW with two clients. We show that SNOW is possible in 
a multi-writer, single-reader (MWSR) setting when a client can send 
messages to other clients with an algorithm; on the other hand, we 
prove it is impossible to implement SNOW in a multi-writer, 
single-reader (MWSR) setting–which is more general than the two-client 
setting–when client-to-client communication is disallowed. We also 
correct the previous claim that incorrectly identified one existing 
system, Eiger, as supporting the strongest guarantees (SW) and whose 
read-only transactions had bounded latency. Thus, there were no 
previous algorithms that provided the strongest guarantees and had 
bounded latency. Finally, we introduce the first two algorithms to 
provide the strongest guarantees with bounded latency.</p> 
		
		</ul>

		<p>(3/12) Algorithmic insights on continual learning from fruit flies: Saket Navlakha</p>
			<ul>
			<strong>Abstract:</strong>
			<p> Continual learning in computational systems is challenging 
due to catastrophic forgetting. We found a two-layer circuit in the 
fruit fly olfactory system that helps to address these challenges by 
uniquely combining two common ideas: sparse coding and associative 
learning. In the first layer, odors are encoded using sparse, 
high-dimensional representations, which reduces memory interference by 
activating non-overlapping populations of neurons for different odors. 
In the second layer, only the synapses between odor-activated neurons 
and the odor’s associated output neuron are modified during learning; 
the rest of the weights are frozen to prevent unrelated memories from 
being overwritten. We show empirically and analytically that this 
simple algorithm boosts continual learning performance compared to 
existing algorithms. We also find that the fly’s associative learning 
algorithm is similar to the classic perceptron learning algorithm, 
albeit two modifications, which we prove are critical for reducing 
catastrophic forgetting.</p> 
		
		</ul>


		<p>(3/26) Thalamocortical contribution to solving credit assignment in neural systems: Brabeeba Wang</p>
			<ul>
			<strong>Abstract:</strong>
			<p> Animal brains evolved to optimize behavior in dynamically changing environments, selecting actions that maximize future rewards. A large body of experimental work indicates that such optimization changes the wiring of neural circuits, appropriately mapping environmental input onto behavioral outputs. A major unsolved scientific question is how optimal wiring adjustments, which must target the connections responsible for rewards, can be accomplished when the relation between sensory inputs, action taken, environmental context with rewards is ambiguous. The computational problem of properly targeting cues, contexts and actions that lead to reward is known as structural, contextual and temporal credit assignment respectively. In this review, we survey prior approaches to these three types of problems and advance the notion that the brain's specialized neural architectures provide efficient solutions. Within this framework, the thalamus with its cortical and basal ganglia interactions serve as a systems-level solution to credit assignment. Specifically, we propose that thalamocortical interaction is the locus of meta-learning where the thalamus provides cortical control functions that parametrize the cortical activity association space. By selecting among these control functions, the basal ganglia hierarchically guide thalamocortical plasticity across two timescales to enable meta-learning. The faster timescale establishes contextual associations to enable rapid behavioral flexibility while the slower one enables generalization to new contexts. Incorporating different thalamic control functions under this framework clarifies how thalamocortical-basal ganglia interactions may simultaneously solve the three credit assignment problems.</p> 
			<li><a href="https://arxiv.org/pdf/2104.01474.pdf">Paper</a> </li>
			<li><a href="./thalamus.pdf">Slides</a> </li>
		
		</ul>

		<p>(4/9) Distributional RL and dopamine circuitry: Brabeeba Wang</p>
			<ul>
				<strong>Abstract:</strong>
			<p> It is well established that the dopamine encodes reward prediction error to facilitate reward seeking learning. However, in recent years, people begin to realize that there is heterogeneity in the dopamine response and the activities of the dopamine can be captured better by the distributional RL framework. In this talk, I am going to talk about the basic neural circuitry of dopamine first and then cover the basic of distributional RL and its connections with dopamine neurons.</p> 
			<strong>Reading list:</strong>
			<ul>
				<li><a href="https://www.sciencedirect.com/science/article/pii/S0166223620301983">Distributional Reinforcement Learning in the Brain</a> </li>
				<li><a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-neuro-072116-031109">Neural Circuitry of Reward Prediction Error</a> </li>
				<li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0092867420315300">A unified framework for dopamine signals across timescales</a> </li>
				<li><a href="https://www.nature.com/articles/s41586-019-1924-6">A distributional code for value in dopamine-based reinforcement learning</a> </li>
			</ul>
			<li><a href="./distributionalrl.pdf">Slides</a> </li>
		
		</ul>

		<p>(4/16) Dopamine representation: Sabrina Drammis</p>
			<ul>
				
			<strong>Reading list:</strong>
			<ul>
				<li><a href="https://www.nature.com/articles/s41586-019-1261-9">Specialized coding of sensory, motor and cognitive variables in VTA dopamine neurons</a> </li>
				<li><a href="https://www.nature.com/articles/s41586-019-1235-y">Dissociable dopamine dynamics for learning and motivation</a> </li>
				<li><a href="https://www.sciencedirect.com/science/article/pii/S2211124720311451">Functional Dissection of Basal Ganglia Inhibitory Inputs onto Substantia Nigra Dopaminergic Neurons</a> </li>
			</ul>
		
		</ul>

		<p>(4/23) Computational overview on different aspects of basal ganglia: Brabeeba Wang</p>
			<ul>
			<strong>Abstract:</strong>
			<p> In this talk, I introduce different important computational aspects of basal ganglia. I first talk about the classical actor critic model of basal ganglia, then talk about how corticostrital plasticity can serves to adjust the threshold of evidence integration. Lastly, I talk about how the choice selective sequence in cortex is able to bridge the past action and outcome to influence future decision through cortico-basal ganglia circuitry.</p> 
			<strong>Reading list:</strong>
			<ul>
				<li><a href="https://www.princeton.edu/~yael/Publications/Niv2009.pdf">Reinforcement learning in the brain (2009)</a> </li>
				<li><a href="https://www.nature.com/articles/nn1722">Cortico–basal ganglia circuit mechanism for a decision threshold in reaction time tasks (2006)</a> </li>
				<li><a href="https://www.biorxiv.org/content/10.1101/725382v2">Choice-selective sequences dominate in cortical relative to thalamic inputs to nucleus accumbens, providing a potential substrate for credit assignment (2020)</a> </li>
				<li><a href="https://www.researchgate.net/profile/Roland-Suri-2/publication/12908688_A_neural_network_model_with_dopamine-like_reinforcement_signal_that_learns_a_spatial_delayed_response_task/links/5beeb779299bf1124fd64c06/A-neural-network-model-with-dopamine-like-reinforcement-signal-that-learns-a-spatial-delayed-response-task.pdf">A neural network model with dopamine-like reinforcement signal that learns a spatial delayed response task (1999)</a> </li>
			</ul>
			<li><a href="./Model of BG.pdf">Slides</a> </li>
		
		</ul>

		<p>(4/30) Laplace code for distributional RL: Brabeeba Wang</p>
			<ul>
			<strong>Abstract:</strong>
			<p> In this talk, I introduce a recent paper on a different way to encode distribution in dopamine neurons. Instead of the original quantile/expectile code, the paper proposes that by varying the discount factor, reward sensitivity and temporal discount factor, one can encode the distribution through a flexible laplace code. Furthermore, this laplace code can be read out from successor representation which is a conjectured model for hippocampus. </p> 
				
			<strong>Reading list:</strong>
			<ul>
				<li><a href="https://proceedings.neurips.cc/paper/2020/file/9dd16e049becf4d5087c90a83fea403b-Paper.pdf">A Local Temporal Difference Code for Distributional Reinforcement Learning (2009)</a> </li>
			</ul>
			<li><a href="./Laplace code.pdf">Slides</a> </li>
		
		</ul>

		<p>(5/7) Why only us? An overview on language: Nancy Lynch</p>
			<ul>
			
			<strong>Reading list:</strong>
			<ul>
				<li><a href="https://mitpress.mit.edu/books/why-only-us">Why only us: Langugae and Evolution. Chomsky and Berwick</a> </li>
			</ul>
		
		</ul>

		<p>(5/14) SuperUROP presentation: Grace Cai, Emily Zhang, Keith Murray</p>
			<ul>
			<strong>Abstract:</strong>
			<p> This week, our three superUROP is giving their presentation. Grace gives a presentation on spatially dependent ant house hunting algorithm. Emily gives a presentation on an analysis on convergence of an ant house hunting algorithm. Keith gives a presentation on a task optimized CNN giving mechanistic insights on direction sensitive retinal circuit.</p> 
		
		</ul>

		<p>(5/21) Biologically motivated computational models of the brain: Lisa Yang</p>
			<ul>
			<li><a href="./TDS_Lisa.pptx">Slides</a> </li>
		
		</ul>

	</ul>
	<a href="https://accessibility.mit.edu/">Accessibility</a>
				
		
		
	<!--	<h4><strong>Unscheduled but need/want to fit in somewhere</strong></h4>
		<p>
			<ul>
				<li><a href="https://igi-web.tugraz.at/people/maass/psfiles/154.pdf">What Can a Neuron Learn with Spike-Timing-Dependent Plasticity?</a> Brabeeba might be interested in presenting.
			<li><a href="https://arxiv.org/abs/1803.09574">Long short-term memory and learning-to-learn in networks of spiking neurons</a>, Quanquan?</li>
			<li><a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004347">Decreasing-Rate Pruning Optimizes the Construction of Efficient and Robust Distributed Networks</a>, Navlakha, Barth, Bar-Joseph.</li>
			<li>Other learning papers people are interested in?</li>
			</ul>
		</p>-->
	  </section>
	</div>
</div>
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../personal_site/js/jquery-1.11.1.min.js"></script>
    <script src="../personal_site/js/bootstrap.min.js"></script>

</div>
</body>
</html>